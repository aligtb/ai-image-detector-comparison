{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d0b9b1",
   "metadata": {},
   "source": [
    "General Evaluation Function for Multiple Models & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a44f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a549b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, model_names, data_loaders, dataset_names, device='cuda'):\n",
    "    \"\"\"\n",
    "    General evaluation function for multiple models across multiple datasets.\n",
    "\n",
    "\n",
    "    Args:\n",
    "    models (list): list of PyTorch models.\n",
    "    model_names (list): list of model names (strings).\n",
    "    data_loaders (list): list of DataLoader objects for datasets.\n",
    "    dataset_names (list): list of dataset names (strings).\n",
    "    device (str): device to run evaluation ('cuda' or 'cpu').\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    results_dict (dict): nested dictionary containing metrics for each model/dataset.\n",
    "    \"\"\"\n",
    "    results_dict = {}\n",
    "\n",
    "\n",
    "    for model, m_name in zip(models, model_names):\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        results_dict[m_name] = {}\n",
    "\n",
    "\n",
    "        for loader, d_name in zip(data_loaders, dataset_names):\n",
    "            y_true, y_pred, y_prob = [], [], []\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    probs = torch.softmax(outputs, dim=1)[:, 1]\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "                    y_true.extend(labels.cpu().numpy())\n",
    "                    y_pred.extend(preds.cpu().numpy())\n",
    "                    y_prob.extend(probs.cpu().numpy())\n",
    "\n",
    "\n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred)\n",
    "        rec = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "        # Store results\n",
    "        results_dict[m_name][d_name] = {\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'roc_auc': auc,\n",
    "        'confusion_matrix': cm\n",
    "        }\n",
    "\n",
    "\n",
    "        # --- Plot confusion matrix ---\n",
    "        plt.figure(figsize=(5,4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Real','AI'], yticklabels=['Real','AI'])\n",
    "        plt.title(f'Confusion Matrix: {m_name} on {d_name}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # --- ROC Curve ---\n",
    "        from sklearn.metrics import roc_curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f'{m_name} on {d_name} (AUC={auc:.2f})')\n",
    "        plt.plot([0,1],[0,1],'--', color='gray')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        # --- Classification report ---\n",
    "        print(f'Classification Report for {m_name} on {d_name}:')\n",
    "        print(classification_report(y_true, y_pred, target_names=['Real','AI']))\n",
    "\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ca6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Usage ---\n",
    "# models = [model1, model2]\n",
    "# model_names = ['ResNet50', 'EfficientNetB0']\n",
    "# data_loaders = [val_loader1, val_loader2]\n",
    "# dataset_names = ['CIFAKE', 'ExtraDataset']\n",
    "# results = evaluate_models(models, model_names, data_loaders, dataset_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
